{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kierstenhenderson/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.learn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/dataset1_shortjokes_uci_cnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for lowercase \n",
    "df.text = df.text.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>funny</th>\n",
       "      <th>not_funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[me narrating a documentary about narrators] \"...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telling my daughter garlic is good for you. go...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i've been going through a really rough period ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if i could have dinner with anyone, dead or al...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>two guys walk into a bar. the third guy ducks.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  funny  not_funny\n",
       "0  [me narrating a documentary about narrators] \"...      1          0\n",
       "1  telling my daughter garlic is good for you. go...      1          0\n",
       "2  i've been going through a really rough period ...      1          0\n",
       "3  if i could have dinner with anyone, dead or al...      1          0\n",
       "4     two guys walk into a bar. the third guy ducks.      1          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>funny</th>\n",
       "      <th>not_funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   funny  not_funny\n",
       "0      1          0\n",
       "1      1          0\n",
       "2      1          0\n",
       "3      1          0\n",
       "4      0          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(6)\n",
    "shuffle = np.random.permutation(np.arange(df.shape[0]))\n",
    "\n",
    "X = df['text']\n",
    "Y_1 = df['funny']\n",
    "Y_2 = df['not_funny'] \n",
    "X, Y_1, Y_2 = X[shuffle], Y_1[shuffle],Y_2[shuffle]\n",
    "Y = pd.DataFrame({\"funny\": Y_1.values,\"not_funny\": Y_2.values})\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (463314,)\n",
      "label shape: (463314, 2)\n",
      "test_data (46331,)\n",
      "dev_data (92663,)\n",
      "train_data (324320,)\n"
     ]
    }
   ],
   "source": [
    "print('data shape: ', X.shape)\n",
    "print('label shape:', Y.shape)\n",
    "\n",
    "\n",
    "test_data, test_label = X[round(X.shape[0]*.9):], Y[round(Y.shape[0]*.9):]\n",
    "dev_data, dev_label = X[round(X.shape[0]*.7):round(X.shape[0]*.9)], Y[round(Y.shape[0]*.7):round(Y.shape[0]*.9)]\n",
    "train_data, train_label = X[:round(X.shape[0]*.7)], Y[:round(Y.shape[0]*.7)]\n",
    "\n",
    "\n",
    "print('test_data', test_data.shape)\n",
    "print('dev_data', dev_data.shape)\n",
    "print('train_data', train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-12-02 03:31:21--  http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.42B.300d.zip [following]\n",
      "--2018-12-02 03:31:22--  https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1877800501 (1.7G) [application/zip]\n",
      "Saving to: ‘glove.42B.300d.zip’\n",
      "\n",
      "glove.42B.300d.zip  100%[===================>]   1.75G  6.75MB/s    in 2m 41s  \n",
      "\n",
      "2018-12-02 03:34:03 (11.1 MB/s) - ‘glove.42B.300d.zip’ saved [1877800501/1877800501]\n",
      "\n",
      "Archive:  glove.42B.300d.zip\n",
      "  inflating: glove.42B.300d.txt      \n"
     ]
    }
   ],
   "source": [
    "#download and upzip Glove embeddings\n",
    "if not os.path.exists('glove.42B.300d.zip'):\n",
    "    ! wget http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
    "    ! unzip glove.42B.300d.zip\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe!\n",
      "400000\n",
      "(400000, 300)\n"
     ]
    }
   ],
   "source": [
    "filename = 'glove.6B.300d.txt'\n",
    "\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadGloVe(filename)\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = len(embd[0])\n",
    "embedding = np.asarray(embd)\n",
    "\n",
    "print(vocab_size)\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = embedding.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-036d2d37c64f>:8: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /home/kierstenhenderson/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /home/kierstenhenderson/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "# Setup vocabulary processor to process embeddings and fit to train, dev, and test data\n",
    "\n",
    "max_document_length = 34\n",
    "\n",
    "#vocab_processor = tflearn.data_utils.VocabularyProcessor(max_length, min_frequency=0)\n",
    "\n",
    "\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "#fit the vocab from glove\n",
    "pretrain = vocab_processor.fit(vocab)\n",
    "#transform inputs\n",
    "#x = np.array(list(vocab_processor.transform(your_raw_input)))\n",
    "x_train = np.array(list(vocab_processor.transform(train_data)))\n",
    "x_dev = np.array(list(vocab_processor.transform(dev_data)))\n",
    "x_test = np.array(list(vocab_processor.transform(test_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(324320, 2)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(train_label).astype(int)\n",
    "y_dev = np.array(dev_label).astype(int)\n",
    "y_test = np.array(test_label).astype(int)\n",
    "\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corresponding ids\n",
      " [   1 1586 3878    5    1   78   19 7102  130    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "corresponding ids\n",
      " [1 0]\n",
      "corresponding ids\n",
      " [ 126 3977    2 1727  711 1585 6630  169  809    5  809  223   10    5\n",
      "  809  303  809    5   10  398   10    5   10    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "corresponding ids\n",
      " [12885    12   576  8868     5     0  2630     1  4679     5   184  1283\n",
      "     4  1233  7394   184 95952    12 11751    25   576  5974     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0]\n",
      "corresponding ids\n",
      " [  289     4     6  7649  2359    69     6  2044 50448 59064     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the worst journey in the world by helen back'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('corresponding ids\\n',x_train[0])\n",
    "print('corresponding ids\\n',y_train[0])\n",
    "\n",
    "print('corresponding ids\\n',x_train[1])\n",
    "print('corresponding ids\\n',x_train[2])\n",
    "print('corresponding ids\\n',x_train[2000])\n",
    "train_data.iloc[0]\n",
    "train_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, pretrained_embedding, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        \n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            \n",
    "            #toggle true and false to train or not train Glove embeddings\n",
    "            \n",
    "            #from preassigned embedding W approach\n",
    "            self.W=tf.get_variable(name=\"embedding_\",shape=pretrained_embedding.shape,initializer=tf.constant_initializer(pretrained_embedding),trainable=False)\n",
    "            \n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                Wname = \"w_%d\"%filter_size\n",
    "                #W = tf.get_variable(Wname, shape = filter_shape, initializer = tf.contrib.layers.xavier_initializer())\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                \n",
    "                b = tf.Variable(tf.constant(0.0, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                \n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                \n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        \n",
    "        #the line below differs from the tutorial, but is necessary based on changes to Tensorflow\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.0, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            print(\"scores shape:\", self.scores.shape)\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            \n",
    "            #correct_predictions = tf.equal(tf.cast(self.predictions,tf.float32), self.input_y)\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "            \n",
    "    \n",
    "        with tf.name_scope('train'):\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(ids, labels, batch_size=100):\n",
    "            \n",
    "    n_batches = len(ids)//batch_size\n",
    "    ids, labels = ids[:n_batches*batch_size], labels[:n_batches*batch_size]\n",
    "    shuffle = np.random.permutation(np.arange(n_batches*batch_size))\n",
    "    ids, labels = ids[shuffle], labels[shuffle]\n",
    "\n",
    "    \n",
    "    for ii in range(0, len(ids), batch_size):\n",
    "        yield ids[ii:ii+batch_size], labels[ii:ii+batch_size]\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pretrained GloVe Embeddings, Switch to trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/kierstenhenderson/Final_project/runs/cnn\n",
      "\n",
      "scores shape: (?, 2)\n",
      "WARNING:tensorflow:From <ipython-input-13-e5969048441e>:101: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "completed cnn creation\n",
      "# batches = 5067\n",
      "Train epoch 0, average loss 1.03662, average accuracy 0.871164,\n",
      "\t\tDev epoch 0, average loss 1.19198, average accuracy 0.89921,\n",
      "\t\t\t\t    Time taken for 0 epochs =  247.29960703849792\n",
      "Train epoch 1, average loss 1.17292, average accuracy 0.887551,\n",
      "\t\tDev epoch 1, average loss 0.668832, average accuracy 0.92167,\n",
      "Train epoch 2, average loss 1.09064, average accuracy 0.894991,\n",
      "\t\tDev epoch 2, average loss 1.18023, average accuracy 0.915558,\n",
      "Train epoch 3, average loss 1.07578, average accuracy 0.900373,\n",
      "\t\tDev epoch 3, average loss 1.17951, average accuracy 0.919413,\n",
      "Train epoch 4, average loss 1.01848, average accuracy 0.902741,\n",
      "\t\tDev epoch 4, average loss 0.867275, average accuracy 0.92884,\n",
      "Train epoch 5, average loss 0.945813, average accuracy 0.90468,\n",
      "\t\tDev epoch 5, average loss 1.00918, average accuracy 0.925168,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-835e0ab4d777>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mtotal_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Actual training loop:\n",
    "embed_dim = 300 \n",
    "filter_sizes= [3,4,5]\n",
    "num_filters = 256\n",
    "l2_reg_lambda = 0\n",
    "learning_rate = 0.01\n",
    "keep_prob = 0.5\n",
    "evaluate_train = 1 # of epochs at which to print test accuracy\n",
    "evaluate_dev = 1 # of epochs at which to estimate and print dev accuracy\n",
    "time_print = 12 # of epochs at which to print time taken\n",
    "num_classes = 2\n",
    "num_epochs = 10\n",
    "\n",
    "num_checkpoints = 1\n",
    "batch_size = 64\n",
    "\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", \"cnn\"))\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        cnn = TextCNN(sequence_length=x_train.shape[1], num_classes=num_classes, vocab_size=V, embedding_size=embed_dim, pretrained_embedding=embedding, filter_sizes= filter_sizes, \n",
    "                      num_filters=num_filters, l2_reg_lambda=l2_reg_lambda)\n",
    "        \n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        print('completed cnn creation')\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "        \n",
    "\n",
    "        print('# batches =', len(x_train)//batch_size)\n",
    "        start = time.time()\n",
    "        for e in range(num_epochs):\n",
    "                    \n",
    "            #sum_scores = np.zeros((batch_size*(len(x_train)//batch_size),1))\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            for i, (x, y) in enumerate(batch_generator(x_train, y_train, batch_size), 1):\n",
    "                feed = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: keep_prob}\n",
    "                _, scores, loss, accuracy = sess.run([cnn.optimizer,cnn.scores,cnn.loss, cnn.accuracy],feed_dict = feed)\n",
    "                total_loss += loss*len(x)\n",
    "                total_acc += accuracy*len(x)\n",
    "                \n",
    "                #sum_scores[i*batch_size:(i+1)*batch_size,:] = scores\n",
    "                #print(np.mean(sum_scores))\n",
    "                #time_str = datetime.datetime.now().isoformat()\n",
    "            if e%evaluate_train==0:\n",
    "                avg_loss = total_loss/(batch_size*(len(x_train)//batch_size))\n",
    "                avg_acc = total_acc/(batch_size*(len(x_train)//batch_size))\n",
    "                print(\"Train epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "                #print('average',np.mean(scores))\n",
    "\n",
    "            if e%evaluate_dev==0:\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                num_batches = 0\n",
    "                for ii, (x, y) in enumerate(batch_generator(x_dev, y_dev, batch_size), 1):\n",
    "                    feed_dict = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: 1.0}\n",
    "                    loss, accuracy = sess.run([cnn.loss, cnn.accuracy],feed_dict)\n",
    "                    total_loss += loss*len(x)\n",
    "                    total_acc += accuracy*len(x)\n",
    "                    num_batches += 1\n",
    "                avg_loss = total_loss/(num_batches*batch_size)\n",
    "                avg_acc = total_acc/(num_batches*batch_size)\n",
    "                #time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"\\t\\tDev epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "            if e%time_print == 0:\n",
    "                end = time.time()\n",
    "                print(\"\\t\\t\\t\\t    Time taken for\",e,\"epochs = \", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pretrained GloVe Embeddings, Switch to trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, pretrained_embedding, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        \n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            \n",
    "            #toggle true and false to train or not train Glove embeddings\n",
    "            \n",
    "            self.W=tf.get_variable(name=\"embedding_\",shape=pretrained_embedding.shape,initializer=tf.constant_initializer(pretrained_embedding),trainable=True)\n",
    "            \n",
    "            \n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                Wname = \"w_%d\"%filter_size\n",
    "                #W = tf.get_variable(Wname, shape = filter_shape, initializer = tf.contrib.layers.xavier_initializer())\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                \n",
    "                b = tf.Variable(tf.constant(0.0, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                \n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                \n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        \n",
    "        #the line below differs from the tutorial, but is necessary based on changes to Tensorflow\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.0, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            print(\"scores shape:\", self.scores.shape)\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            \n",
    "            #correct_predictions = tf.equal(tf.cast(self.predictions,tf.float32), self.input_y)\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "          \n",
    "        with tf.name_scope('train'):\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/kierstenhenderson/Final_project/runs/cnn\n",
      "\n",
      "scores shape: (?, 2)\n",
      "completed cnn creation\n",
      "# batches = 5067\n",
      "Train epoch 0, average loss 0.781189, average accuracy 0.924854,\n",
      "\t\tDev epoch 0, average loss 0.764221, average accuracy 0.944465,\n",
      "\t\t\t\t    Time taken for 0 epochs =  1812.7710220813751\n",
      "Train epoch 1, average loss 0.745564, average accuracy 0.929387,\n",
      "\t\tDev epoch 1, average loss 1.42296, average accuracy 0.946916,\n",
      "Train epoch 2, average loss 0.802375, average accuracy 0.925125,\n",
      "\t\tDev epoch 2, average loss 1.35228, average accuracy 0.945113,\n",
      "Train epoch 3, average loss 0.845683, average accuracy 0.923855,\n",
      "\t\tDev epoch 3, average loss 1.74177, average accuracy 0.936679,\n"
     ]
    }
   ],
   "source": [
    "#Actual training loop:\n",
    "\n",
    "embed_dim = 300 \n",
    "filter_sizes= [3,4,5]\n",
    "num_filters = 256\n",
    "l2_reg_lambda = 0\n",
    "learning_rate = 0.01\n",
    "keep_prob = 0.5\n",
    "evaluate_train = 1 # of epochs at which to print test accuracy\n",
    "evaluate_dev = 1 # of epochs at which to estimate and print dev accuracy\n",
    "time_print = 12 # of epochs at which to print time taken\n",
    "num_classes = 2\n",
    "num_epochs = 10\n",
    "\n",
    "num_checkpoints = 1\n",
    "batch_size = 64\n",
    "\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", \"cnn\"))\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        cnn = TextCNN(sequence_length=x_train.shape[1], num_classes=num_classes, vocab_size=V, embedding_size=embed_dim, pretrained_embedding=embedding, filter_sizes= filter_sizes, \n",
    "                      num_filters=num_filters, l2_reg_lambda=l2_reg_lambda)\n",
    "        \n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        \n",
    "        print('completed cnn creation')\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "        \n",
    "\n",
    "        print('# batches =', len(x_train)//batch_size)\n",
    "        start = time.time()\n",
    "        for e in range(num_epochs):\n",
    "                    \n",
    "            #sum_scores = np.zeros((batch_size*(len(x_train)//batch_size),1))\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            for i, (x, y) in enumerate(batch_generator(x_train, y_train, batch_size), 1):\n",
    "                feed = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: keep_prob}\n",
    "                _, scores, loss, accuracy = sess.run([cnn.optimizer,cnn.scores,cnn.loss, cnn.accuracy],feed_dict = feed)\n",
    "                total_loss += loss*len(x)\n",
    "                total_acc += accuracy*len(x)\n",
    "                \n",
    "                #sum_scores[i*batch_size:(i+1)*batch_size,:] = scores\n",
    "                #print(np.mean(sum_scores))\n",
    "                #time_str = datetime.datetime.now().isoformat()\n",
    "            if e%evaluate_train==0:\n",
    "                avg_loss = total_loss/(batch_size*(len(x_train)//batch_size))\n",
    "                avg_acc = total_acc/(batch_size*(len(x_train)//batch_size))\n",
    "                print(\"Train epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "                #print('average',np.mean(scores))\n",
    "\n",
    "            if e%evaluate_dev==0:\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                num_batches = 0\n",
    "                for ii, (x, y) in enumerate(batch_generator(x_dev, y_dev, batch_size), 1):\n",
    "                    feed_dict = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: 1.0}\n",
    "                    loss, accuracy = sess.run([cnn.loss, cnn.accuracy],feed_dict)\n",
    "                    total_loss += loss*len(x)\n",
    "                    total_acc += accuracy*len(x)\n",
    "                    num_batches += 1\n",
    "                avg_loss = total_loss/(num_batches*batch_size)\n",
    "                avg_acc = total_acc/(num_batches*batch_size)\n",
    "                #time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"\\t\\tDev epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "            if e%time_print == 0:\n",
    "                end = time.time()\n",
    "                print(\"\\t\\t\\t\\t    Time taken for\",e,\"epochs = \", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pretrained GloVe Embeddings, Switch to trainable = True, change Learning Rate from 0.01 to 0.001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/kierstenhenderson/Final_project/runs/cnn\n",
      "\n",
      "scores shape: (?, 2)\n",
      "WARNING:tensorflow:From <ipython-input-15-534eca9bec5b>:97: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "completed cnn creation\n",
      "# batches = 5067\n",
      "Train epoch 0, average loss 0.310542, average accuracy 0.886101,\n",
      "\t\tDev epoch 0, average loss 0.118627, average accuracy 0.956645,\n",
      "\t\t\t\t    Time taken for 0 epochs =  1899.1284637451172\n",
      "Train epoch 1, average loss 0.111261, average accuracy 0.958568,\n",
      "\t\tDev epoch 1, average loss 0.0815744, average accuracy 0.969625,\n",
      "Train epoch 2, average loss 0.0715139, average accuracy 0.973289,\n",
      "\t\tDev epoch 2, average loss 0.0673643, average accuracy 0.97497,\n",
      "Train epoch 3, average loss 0.0514264, average accuracy 0.981233,\n",
      "\t\tDev epoch 3, average loss 0.0619873, average accuracy 0.977518,\n",
      "Train epoch 4, average loss 0.0385358, average accuracy 0.98609,\n",
      "\t\tDev epoch 4, average loss 0.0608853, average accuracy 0.978749,\n",
      "Train epoch 5, average loss 0.0293575, average accuracy 0.989278,\n",
      "\t\tDev epoch 5, average loss 0.061461, average accuracy 0.979278,\n",
      "Train epoch 6, average loss 0.0228706, average accuracy 0.991819,\n",
      "\t\tDev epoch 6, average loss 0.0648446, average accuracy 0.978976,\n",
      "Train epoch 7, average loss 0.0178829, average accuracy 0.993598,\n",
      "\t\tDev epoch 7, average loss 0.0669616, average accuracy 0.979667,\n",
      "Train epoch 8, average loss 0.014191, average accuracy 0.994995,\n",
      "\t\tDev epoch 8, average loss 0.0714255, average accuracy 0.979505,\n",
      "Train epoch 9, average loss 0.0113403, average accuracy 0.995967,\n",
      "\t\tDev epoch 9, average loss 0.0759151, average accuracy 0.97984,\n"
     ]
    }
   ],
   "source": [
    "#Actual training loop:\n",
    "\n",
    "embed_dim = 300 \n",
    "filter_sizes= [3,4,5]\n",
    "num_filters = 256\n",
    "l2_reg_lambda = 0\n",
    "learning_rate = 0.0001\n",
    "keep_prob = 0.5\n",
    "evaluate_train = 1 # of epochs at which to print test accuracy\n",
    "evaluate_dev = 1 # of epochs at which to estimate and print dev accuracy\n",
    "time_print = 12 # of epochs at which to print time taken\n",
    "num_classes = 2\n",
    "#num_epochs = 61\n",
    "num_epochs = 10\n",
    "\n",
    "num_checkpoints = 1\n",
    "batch_size = 64\n",
    "\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", \"cnn\"))\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        cnn = TextCNN(sequence_length=x_train.shape[1], num_classes=num_classes, vocab_size=V, embedding_size=embed_dim, pretrained_embedding=embedding, filter_sizes= filter_sizes, \n",
    "                      num_filters=num_filters, l2_reg_lambda=l2_reg_lambda)\n",
    "        \n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        \n",
    "        print('completed cnn creation')\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "       \n",
    "\n",
    "        print('# batches =', len(x_train)//batch_size)\n",
    "        start = time.time()\n",
    "        for e in range(num_epochs):\n",
    "                    \n",
    "            #sum_scores = np.zeros((batch_size*(len(x_train)//batch_size),1))\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            for i, (x, y) in enumerate(batch_generator(x_train, y_train, batch_size), 1):\n",
    "                feed = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: keep_prob}\n",
    "                _, scores, loss, accuracy = sess.run([cnn.optimizer,cnn.scores,cnn.loss, cnn.accuracy],feed_dict = feed)\n",
    "                total_loss += loss*len(x)\n",
    "                total_acc += accuracy*len(x)\n",
    "                \n",
    "                #sum_scores[i*batch_size:(i+1)*batch_size,:] = scores\n",
    "                #print(np.mean(sum_scores))\n",
    "                #time_str = datetime.datetime.now().isoformat()\n",
    "            if e%evaluate_train==0:\n",
    "                avg_loss = total_loss/(batch_size*(len(x_train)//batch_size))\n",
    "                avg_acc = total_acc/(batch_size*(len(x_train)//batch_size))\n",
    "                print(\"Train epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "                #print('average',np.mean(scores))\n",
    "\n",
    "            if e%evaluate_dev==0:\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                num_batches = 0\n",
    "                for ii, (x, y) in enumerate(batch_generator(x_dev, y_dev, batch_size), 1):\n",
    "                    feed_dict = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: 1.0}\n",
    "                    loss, accuracy = sess.run([cnn.loss, cnn.accuracy],feed_dict)\n",
    "                    total_loss += loss*len(x)\n",
    "                    total_acc += accuracy*len(x)\n",
    "                    num_batches += 1\n",
    "                avg_loss = total_loss/(num_batches*batch_size)\n",
    "                avg_acc = total_acc/(num_batches*batch_size)\n",
    "                #time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"\\t\\tDev epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "            if e%time_print == 0:\n",
    "                end = time.time()\n",
    "                print(\"\\t\\t\\t\\t    Time taken for\",e,\"epochs = \", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
